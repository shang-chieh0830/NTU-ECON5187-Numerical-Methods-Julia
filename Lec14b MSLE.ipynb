{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b47619b",
   "metadata": {},
   "source": [
    "# Maximum Simulated Likelihood Estimation Using SFM as Examples\n",
    "\n",
    "\n",
    "$$\\require{color}\n",
    "\\definecolor{purple}{RGB}{114,0,172}\n",
    "\\definecolor{green}{RGB}{45,177,93}\n",
    "\\definecolor{red}{RGB}{251,0,29}\n",
    "\\definecolor{blue}{RGB}{18,110,213}\n",
    "\\definecolor{orange}{RGB}{217,86,16}\n",
    "\\definecolor{pink}{RGB}{203,23,206}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab37fe",
   "metadata": {},
   "source": [
    "## What's it and Why?\n",
    " \n",
    "\n",
    "- MSLE is an extension of MLE. It is often used in lieu of MLE when the likelihood function of the model cannot be derived analytically. In many cases, it happens when the model's density function involves an integral that cannot be expressed in closed form. We may call such a model _analytically intractable_. In this case, the likelihood function is approximated through simulation methods.\n",
    "\n",
    "\n",
    "- Why we may care for analytically intractable models? Why not settle only for the tractable models?\n",
    "\n",
    "  - Analytically tractable models are easier to estimate, but the simplicity is often achieved through rigid behavioral assumptions.\n",
    "\n",
    "  - For instance, the stochastic frontier models are tractable if the distribution assumptions are \\{normal, truncated normal\\}, \\{normal, exponential\\}, etc.. However, the models become intractable for many other distribution combinations.\n",
    "\n",
    "  - As another example, the multinomial logit model is tractable and easy to estimate, but it needs to impose the independence of irrelevant alternatives (IIA) assumption, which may not easy to justify. \n",
    "><p style=\"line-height:110%; font-size:92%;margin-left:10%\"> The IIA assumption states that the probability of choosing one alternative over another do not depend on the presence or absence of any other alternatives in the choice set. In other words, the relative preference between any two alternatives is independent of the attributes or existence of any other alternatives in the choice set. </p>\n",
    "  - Moreover, models with unobserved heterogeneity and/or  random effects often contain integrals that cannot be solved analytically.\n",
    "\n",
    "  - Therefore, if we want to relax the assumptions, we will have to use the MSLE to deal with the intractable models.\n",
    "\n",
    "\n",
    "\n",
    "- The basic idea of MSLE is to simulate data (generate a series of random draws, known as Monte Carlo draws, from predetermined distributions that represent the characteristics of the model) for the model based on a given set of parameters. The data is then used to construct and approximate the likelihood (i.e., the _simulated likelihood_) function in the estimation. When the parameters are changed in the optimization process, a different set of data is genereated and the model is estimated again. This process continues until the optimization process is done.\n",
    "\n",
    "\n",
    "- In a nutshell, MSLE replaces the integral with a numerically approximated integral.\n",
    "\n",
    "  - As we have learned, there are different methods of numerical integrations, including the quadrature method, the Monte Carlo (MC) method, and the quasi-Monte Carlo (QMC) method. The term of \"MSLE\" is usually reserved for cases using the MC and QMC methods.\n",
    "  \n",
    "  - Nevertheless, the quadrature method could be used in dealing with the intractable likelihoods. At least for the one-dimensional integration problems, the quadrature method has superior performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- MSLE's asymptotic distribution is the same as MLE's if $S \\rightarrow \\infty$, $N \\rightarrow \\infty$, and $\\sqrt{N}/S \\rightarrow 0$ where $S$ is the number of simulation draws.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7450db9c",
   "metadata": {},
   "source": [
    "## Stochastic Frontier Models: Examples\n",
    "\n",
    "\n",
    "A typical parametric stochastic frontier (SF) model may be represented\n",
    "by \n",
    "\n",
    "\\begin{aligned}\n",
    "  y_i & = x_i \\beta + \\varepsilon_i, \\\\\n",
    "  \\varepsilon_i & = v_i - u_i, \\\\\n",
    "  v_i & \\sim N(0,\\sigma_v^2), \\quad \\mbox{(most commonly used)} \\\\\n",
    "  u_i & \\sim  \\mbox{some distribution such that $u_i \\ge 0$},\n",
    "\\end{aligned}\n",
    "\n",
    "where $v_i$ and $u_i$ are independent to each other. The likelihood-based estimation requires deriving the PDF of the model. Given the independence assumption, the PDF of the $i$th observation is\n",
    "\n",
    "\\begin{align}\n",
    "f(\\varepsilon_i \\mid \\theta)  = \\int_0^\\infty f_{v,u}(\\underbrace{\\varepsilon_i + u_i}_{\\color{red} =v_i}, u_i \\mid \\theta)\\, d u_i \n",
    "    \\underbrace{=}_{indep} \\int_0^\\infty f_v(\\varepsilon_i + u_i \\mid \\theta) f_u(u_i \\mid \\theta)\\, d u_i,\\label{eq:e_density}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "   \n",
    "where $f_{v,u}(\\cdot)$ is the joint distribution of $v$ and $u$ and\n",
    "$\\theta = (\\beta, \\sigma_v^2, \\ldots)$ is the vector of parameters of this model. Given the\n",
    "independence of $v_i$ and $u_i$, $f_{v,u}(\\varepsilon_i+ \n",
    "u_i, u_i \\mid \\theta)$ would be a product of the two random variables'\n",
    "density functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6351a",
   "metadata": {},
   "source": [
    "## Conventional MLE Approach \n",
    "\n",
    "If we derive the PDF of $\\varepsilon_i$ in closed form, we may estimate the model's parameters via MLE. In the case of $u_i \\sim N^+(\\mu, \\sigma_u^2)$ where $N^+(\\cdot)$ represents a positive truncation from the underlying normal distribution, the PDFof $\\varepsilon_i$ is:\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "f_v(v|\\theta) & = \\frac{1}{\\sqrt{2 \\pi} \\sigma_v}\n",
    "\\exp\\left[ -\\frac{1}{2} \\left( \\frac{v}{\\sigma_v}  \\right)^2 \\right],\\\\\n",
    "f_u(u|\\theta) &  = \\frac{1}{\\sqrt{2 \\pi} \\sigma_u} \\frac{1}{\\Phi\\left(\\frac{\\mu}{\\sigma_u}\\right)}\n",
    "\\exp\\left[ -\\frac{1}{2} \\left( \\frac{u-\\mu}{\\sigma_u}  \\right)^2 \\right],\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mbox{ } & \\\\\n",
    "f_{v,u}(\\varepsilon + u, u | \\theta)  & =\n",
    "f_v(\\varepsilon_i + u_i \\mid \\theta) f_u(u_i \\mid \\theta) \\\\\n",
    "\\mbox{ } & \\\\\n",
    "& = \\frac{\\exp\\left[ -\\frac{1}{2} \\left(\\frac{\\mu+\\varepsilon}{\\sigma_v}  \\right)^2 + \\left( \\frac{u-\\mu}{\\sigma_u} \\right)^2  \\right]}{2 \\pi \\sigma_v \\sigma_u \\Phi\\left(\\frac{\\mu}{\\sigma_u} \\right)},\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mbox{ } \\notag \\\\\n",
    "f(\\varepsilon_i\\mid \\theta) = \\int_0^\\infty f_{v,u}(\\varepsilon_i + u_i, u_i \\mid \\theta)\\, d u_i = \\frac{\\exp\\left[ -\\frac{1}{2}\\left(\\frac{ \\mu + \\varepsilon_i}{\\sqrt{\\sigma_v^2 + \\sigma_u^2}}\\right)^2\n",
    "    \\right]}{\\sqrt{2\\pi}\\sqrt{\\sigma_v^2+ \\sigma_u^2}\\left[ \\frac{\\Phi\\left(\\frac{\\mu}{\\sigma_u} \\right)}{\\Phi\\left(\\frac{\\mu_*}{\\sigma_*} \\right)} \\right]  },\\label{eq:LL_mle}\n",
    "\\end{align}\n",
    "    \n",
    "where \n",
    "\n",
    "\\begin{align}\n",
    "\\mu_*  = \\frac{ \\mu \\sigma_v^2-\\varepsilon_i \\sigma_u^2}{\\sigma_v^2+ \\sigma_u^2},\\quad\n",
    "\\sigma_*^2  = \\frac{\\sigma_v^2\\sigma_u^2}{\\sigma_v^2+ \\sigma_u^2}.\\label{eq:sigs}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Let $l_i(\\theta \\mid \\varepsilon_i)$ be the likelihood function of the\n",
    "$i$th observation. Given that $l_i(\\theta \\mid \\varepsilon_i) \n",
    "\\propto f(\\varepsilon_i \\mid \\theta)$, the model's parameters may be\n",
    "estimated by maximizing the following log-likelihood function of the\n",
    "model: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "  \\ln L = \\sum_{i=1}^N \\ln f(\\varepsilon_i \\mid \\theta ).\n",
    "\\end{aligned}$$\n",
    "\n",
    "A challenge of the MLE approach is in deriving the density function of $\\varepsilon_i = v_i - u_i$ in the closed form, e.g., \\eqref{eq:LL_mle}. Some distribution combinations of $v_i$ and $u_i$ either do not have the closed forms or are very difficult to derive. The constraint has in some sense limited the development of the SF literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f8a795",
   "metadata": {},
   "source": [
    "## Monte Carlo Simulation Approach (MCSA) \n",
    "\n",
    "\n",
    "To understand MSLE, we re-write \\eqref{eq:e_density} using the conditional density:\n",
    "\n",
    "\\begin{align}\n",
    "   f(\\varepsilon_i \\mid \\theta)  = \\int_0^\\infty f_{v,u}(\\varepsilon_i + u_i, u_i \\mid \\theta)\\, d u_i,  \n",
    "     & = \\int_0^\\infty f_{v|u}(\\varepsilon_i + u_i \\mid \\theta, u_i) f_u(u_i \\mid \\theta)\\, d u_i \\notag \\\\\n",
    "     & = \\mbox{E}_{f_u(u \\mid \\theta)} \\left[ f_{v|u}(\\varepsilon_i + u_i \\mid \\theta, u_i)  \\right ],\\label{eq:sum_msle}\n",
    "\\end{align}\n",
    "     \n",
    "where the expectation is over $u_i$ which has the density $f_u(u_i \\mid \\theta)$. Thus, the log-likelihood of the model may be represented by the sum of the log ofÂ \\eqref{eq:sum_msle} over $i$: \n",
    "\n",
    "\\begin{align}\n",
    "\\ln L & = \\sum_{i=1}^N \\ln \\mbox{E}_{f_u(u \\mid \\theta)} \\left[ f_{v|u}(\\varepsilon_i + u_i \\mid \\theta, u_i)  \\right ] \\label{eq:LL_MSLE}.\n",
    "\\end{align}\n",
    "   \n",
    "   \n",
    "In the estimation, the expected value is approximated by its empirical\n",
    "counterpart:\n",
    "\n",
    "\\begin{align}\n",
    "\\mbox{E}_{f_u(u \\mid \\theta)} \\left[ f_{v|u}(\\varepsilon_i + u_i \\mid \\theta, u_i)  \\right] \\approx\n",
    "  \\frac{1}{S}\\sum_{s=1}^S  f_{v|u}(\\varepsilon_i + u_i^s \\mid \\theta, u_i^s),\\label{eq:LL_msle}\n",
    "\\end{align}  \n",
    "  \n",
    "  \n",
    "where $u_i^s$ is the $s$th element of the random sample $\\mathbf{u}_i^S = (u_i^1, u_i^2, \\ldots, u_i^S)$ drawn from the distribution of $u_i$. Given $\\mathbf{u}_i^S$, \\eqref{eq:LL_msle} is easy to calculate since $f_{v|u}(\\cdot)$ is the PDF of $v_i$ which is often assumed to follow a normal distribution. Maximizing the model inÂ \\eqref{eq:LL_MSLE} andÂ \\eqref{eq:LL_msle} yields MSLE estimates of the model.\n",
    "\n",
    "\n",
    ">> The above estimation method may look complicated. However, it could be understood in a rather intuitive way. Note that the model is\n",
    ">>\n",
    ">> \\begin{aligned}\n",
    "  y_i = x_i' \\beta + \\underbrace{v_i - u_i}_{=\\epsilon_i}.\n",
    " \\end{aligned}\n",
    ">> \n",
    ">> Though $u_i$ is unobservable, we could _simulate_ its characteristics by drawing many values from its distribution. Let $u_i^s$ be the $s$th drawn value. Since $u_i^s$ is a drawn value and no longer random, we re-write the model as:\n",
    ">> \n",
    ">> \\begin{align}\n",
    " y_i  & = (x_i' \\beta - u_i^s) + v_i.\\label{eq:eq1} \\\\\n",
    " y_i - x_i' \\beta + u_i^s & = v_i,\\\\\n",
    " \\epsilon_i + u_i^s & = v_i\n",
    " \\end{align}\n",
    ">> \n",
    ">> Here, \\eqref{eq:eq1} only has one random variable in the model, $v_i$, which is often assumed to be normally distributed. So, we end up with a model that **_looks like_ one that has a normally distributed random variable**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54612227",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\", style=\"max-width: 100%;\">\n",
    "\n",
    "**Formally, the estimation may be described by the following steps. (Assume that $v_i \\sim N(0, \\sigma_v^2)$.)**    \n",
    "    \n",
    "    \n",
    "1. Calculate the likelihood value of a given $i$ (i.e., \\eqref{eq:LL_msle}):\n",
    "\n",
    "  - Note that $\\varepsilon + u_i = v_i$ which has the assumed normal distribution.\n",
    "      \n",
    "  - Compute $\\hat{\\varepsilon}_i + u_i^s$:\n",
    "        \n",
    "    - $\\hat{\\varepsilon}_i = y_i - f(x_i; \\hat{\\beta})$;\n",
    "        \n",
    "    - $u_i^s$ is the $s$th draw from the distribution of $u_i$;\n",
    "    \n",
    "  - Compute $f_{v|u}(\\hat{\\varepsilon}_i + u_i^s)$ where the $f(\\cdot)$ is the density function of $v_i$'s distribution (because $\\varepsilon_i + u_i = v_i$). We have assumed $v_i$ to be normally distributed.\n",
    "    \n",
    "    - For instance, if $\\hat{\\varepsilon}_i = 0.5$ and $u_i^s = 0.1$, then the value is calculated by the following code.\n",
    "    \n",
    "    ```julia\n",
    "    using Distributions\n",
    "    f(e, sigma_v) = pdf(Normal(0, sigma_v), e)     \n",
    "    f(0.5 + 0.1, sigma_v)  # this!\n",
    "    ```    \n",
    "  - Compute $\\frac{1}{S}\\sum_{s=1}^S f_{v|u}(\\hat{\\varepsilon}_i + u_i^s)$ which is the simulated likelihood value of the $i$th observation.\n",
    "    \n",
    "    - For instance, if $\\hat{\\varepsilon}_i = 0.5$, then the value may be calculated by the following code.\n",
    "    \n",
    "    ```julia\n",
    "    # draw `u_list` as a S-element vector from the dist of u_i \n",
    "    sum(f.(0.5 .+ u_list, sigma_v))/S\n",
    "    ```\n",
    "    \n",
    "2. Calculate the log-likelihood value of a given $i$.\n",
    "\n",
    "  - Take log on the value calculated from Step 1.   \n",
    "    \n",
    "    ```julia\n",
    "    log(sum(f.(0.5 .+ u_list, sigma_v))/S)\n",
    "    ```      \n",
    "   \n",
    "3. Calculate the log-likelihood value of the model (i.e., \\eqref{eq:LL_MSLE}).\n",
    "\n",
    "  - Repeat Step 1 & 2 for all the observations ($i=1,\\ldots,N$) and add them up.\n",
    "    \n",
    "    ```julia    \n",
    "    logLike = Array{Real}(undef, size(y,1))\n",
    "    for i in 1:size(y,1) \n",
    "       logLike[i] = log(sum(f.(Ïµ[i,1] .+ u_list, sigma_v)/S))\n",
    "    end\n",
    "    sum(logLike)  # better than running sum\n",
    "    ```        \n",
    "   \n",
    "  - You should use the same set of $u_i^s$ for a given observation $i$ during the iteration. It avoids chatters.\n",
    "  - On the other hand, you could use different draws for different observation $i$. That would improve efficiency.\n",
    "  - ååobservationç¨åæ¨£ç set of $u_i^s$, ä¸åobservationå¯ç¨ä¸åç set of $u_i^s$\n",
    "  - ä¸èª²èªªçè¦ç¨åä¸å random vector, æçæ¯ä½ç²æåºå±¤çé£åé¨æ©æ¸åçé£å vector. ä»¥ inverse transform sampling çºä¾ï¼å®å¨çææå¾ç random draw of u ä¹åï¼éè¦åæä¸å U(0,1) ç vector, éåå°±æ¯æåºå±¤çé£åé¨æ©æ¸åãå°æ¼ä¸å i, éåæ¸åå¨æ¯ä¸æ¬¡ç iteration æå¥½é½ä¸è¦è®ãæäºéåé¨æ©æ¸åï¼åå¥ç¨å° inverse transform sampling ä¹å¾ï¼æ½åºä¾ç u  ç¶ç¶å°±æé¨ç sigma_u^2 çä¸åèæå·®ç°ï¼æä»¥å©ä»¶äºæ²æçç¾ã\n",
    "\n",
    "  - å¦å¤ï¼ æç¾å¨ä¹æ³éäºï¼çºä»éº¼æäºåå­¸çç¨å¼ç¡æ³æ¶ææï¼è½äºæçå»ºè­°æ Xoshiro(..) å å¦å°±å¯ä»¥æ¶æï¼å çºå å¥éå RNGï¼ææ¯çæ­£ç¢ºä¿äºãæ¯ä¸æ¬¡çä¼°è¨ï¼é½è¦ç¨åä¸å random vector ãéä»¶äºãå¦ææ²æéå RNGï¼é£éº¼æ¯æ¬¡çæ U(0,1) çæåï¼æ¯æ¬¡é½æä¸ä¸æ¨£ï¼éæ¨£å°±ä¸å®¹ææ¶æäºã\n",
    "\n",
    "\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd0650",
   "metadata": {},
   "source": [
    "In terms of estimation, the key step here is to get $\\mathbf{u}_i^S$.\n",
    "Econometric softwares may have provided random number generators (RNG)\n",
    "for some distributions. For instance, the RNGs for uniform and normal\n",
    "random variables are standard in most of the software. However, it is\n",
    "rarely a good idea to rely on these RNG to draw $\\mathbf{u}_i^S$ for\n",
    "MSLE, for two reasons. First, the RNGs of some distributions are not\n",
    "available in many software. Second and more importantly, software' RNGs\n",
    "(such as Stata's `runiform()` and `rnormal()`) usually provide\n",
    "pseudo-random numbers (as opposed to quasi-random numbers) which are\n",
    "inefficient when used in the simulationsÂ (taking many more draws\n",
    "to achieve a given precision level).\n",
    "\n",
    "The better and more efficient method is to use quasi-random numbers. It could be accomplished using the inverse transform sampling method coupled with low discrepancy sequences. To illustrate, let $\\Psi(u_i) \\in (0,1)$ be the CDF of $u_i$. Then, we could draw a value of $u_i$ by $u_i^s = \\Psi^{-1}(m)$ where $m \\sim \\mbox{Uniform}(0,1)$ and $\\Psi^{-1}(\\cdot)$ is the inverse CDF (also called the quantile function) of $u_i$. The Halton sequence, which is one type of low discrepancy sequences, could be used for $m$. Because the sequence is generated in a deterministic and strategic manner (thus,*quasi-*random) that it covers the $(0,1)$ more evenly for a given $S$, the simulation can be performed more efficiently. The limitation is that the inverse function has to be available in an appropriate form such that we could substitute $m$ for the Halton sequence and obtain the quasi-random sample of $\\mathbf{u}_i^S$. Unfortunately, the requirement is not easily met for some distributions in many of the software packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1b366",
   "metadata": {},
   "source": [
    "### Numerical Integration Approach (NIA) \n",
    "\n",
    "This approach is intuitive in that it simply numerically evaluates the integration inÂ \\eqref{eq:e_density} to obtain the density of $\\varepsilon_i$. Let $f^*(\\varepsilon_i \\mid \\theta)$ be such a density. Then, the model's log-likelihood function may be represented by \n",
    "\n",
    "\\begin{aligned}\n",
    "  \\ln L = \\sum_{i=1}^N \\ln f^*(\\varepsilon_i \\mid \\theta).\n",
    "\\end{aligned}\n",
    "\n",
    "Compared to the conventional MLE, there is no need of deriving the\n",
    "model's joint distribution in closed form as shown\n",
    "inÂ \\eqref{eq:LL_mle} toÂ \\eqref{eq:sigs}. Without such a burden, all the distribution\n",
    "assumptions for $v_i$ and $u_i$ ($>0$) could be used to construct\n",
    "$\\varepsilon_i = v_i - u_i$; all we need is the PDF of $v_i$ and $u_i$.\n",
    "The numerical integration procedure is invariant to the choice of\n",
    "distribution assumptions.\n",
    "\n",
    "Compared to MCSA, there is no need to have the inverse CDF function for\n",
    "$u_i$ (because we integrate out $u_i$ !!, no need to draw random sample of $u_i$ now), making NIA widely applicable to different distributional choices.\n",
    "\n",
    "\n",
    "There are a few methods of doing numerical integrations in NIA which\n",
    "could be modified/used for SF model estimation. We introduce two of them\n",
    "here: the Gaussian quadrature method and the (quasi) Monte Carlo\n",
    "integration method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d851a0a",
   "metadata": {},
   "source": [
    "#### The Gaussian Quadrature Method (NIA-GQ)\n",
    "\n",
    "The quadrature method uses a set of chosen points (*weights* and\n",
    "*nodes*) to evaluated the function and to approximate the integration.\n",
    "The quadrature-rule based integration approximation takes the following\n",
    "general form:\n",
    "\n",
    "\\begin{align}\n",
    "I = \\int_\\Omega p(u)f(u)\\, du \\approx \\sum_{j=1}^n \\omega_j f(\\xi_j),\\label{eq:quad}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $\\xi_j$s are nodes and $\\omega_j$s are the corresponding weights.\n",
    "Many integration methods fall into this category, including the\n",
    "trapezoidal rule, the Simpson rule, etc.. Here, we consider the\n",
    "*Gaussian* quadrature rules which are designed in a way such that $n$\n",
    "quadrature nodes would integrate a polynomial of degree $2n-1$ exactly.\n",
    "\n",
    "With Gaussian quadratures, integrations with different domain ($\\Omega$)\n",
    "and/or different weight function $p(u)$ may require different sets of\n",
    "the node-weight combinations (called *rules*). The often-used rules\n",
    "include the Gauss-Legendre, Gauss-Hermite, and the Gauss-Laguerre rules.\n",
    "For the purpose of SF analysis where $u_i \\geq 0$, we have\n",
    "$\\Omega = (0,\\, \\infty)$ and the appropriate quadrature rule would be\n",
    "the Gauss-Laguerre rule to which the $p(u)$ function is $e^{-u}$. Then a\n",
    "SF model could be estimated by (see\n",
    "alsoÂ \\eqref{eq:e_density}): \n",
    "\n",
    "\\begin{align}\n",
    "  f(\\varepsilon_i \\mid \\theta) & = \\int_0^\\infty f_v(\\varepsilon_i + u_i\\mid \\theta,) f_u(u_i\\mid \\theta)\\, d u_i \\notag \\\\\n",
    "           & = \\int_0^\\infty e^{-u_i} \\tilde{f}_{v,u}(\\varepsilon_i, u_i \\mid \\theta)\\, d u_i \n",
    "            \\approx \\sum_{j=1}^n \\omega_j \\tilde{f}_{v,u}(\\varepsilon_i, \\xi_j \\mid \\theta),\\label{eq:GL}\n",
    "\\end{align}\n",
    "            \n",
    "where\n",
    "\n",
    "\\begin{aligned}\n",
    "\\tilde{f}_{v,u}(\\varepsilon_i, u_i\\mid \\theta)  = e^{u_i} f_v(\\varepsilon_i + u_i \\mid \\theta) f_u(u_i \\mid \\theta),\n",
    "\\end{aligned}\n",
    "\n",
    "and $(\\omega_j, \\xi_j)$ are the Gauss-Laguerre rules. The model's log-likelihood function may be represented by the sum ofÂ \\eqref{eq:GL} over the observations.\n",
    "\n",
    "The quadrature method is fast and accurate, making it ideal at least for one-dimensional problems. The downside of the method is the curse of dimensionality: For multi-dimensional problems, the method's convergence rate quickly deteriorates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ce50e",
   "metadata": {},
   "source": [
    "#### The Monte Carlo Integration Method (NIA-MCI)\n",
    "\n",
    "Alternative to the quadrature method is the Monte Carlo integration\n",
    "which is based on the simple approximation: \n",
    "\n",
    "\\begin{aligned}\n",
    "I =  \\int_0^1 f(t)\\, dt  =  \\mbox{E}[f(t)]  \n",
    "       \\approx \\frac{1}{S}\\sum_{s=1}^S f(t^s),\n",
    "\\end{aligned}\n",
    "       \n",
    "where $t^s$ is the $s$th element of the sample $\\mathbf{t}^S \n",
    "= (t^1, t^2, \\ldots, t^S)$ drawn from a uniform distribution in\n",
    "$(0,\\,1)$. This example's $t$ has a domain in $(0,\\,1)$ so that it can\n",
    "translate directly to the expected value expression. To apply the method\n",
    "to a SF model, we use a change of variables to change the model's\n",
    "integrator from $u \\in (0,\\,\\infty)$ to $t \\in (0,\\,1)$. To wit, \n",
    "\n",
    "\\begin{align}\n",
    "f(\\varepsilon_i \\mid \\theta) & = \\int_0^\\infty f_{v,u}(\\varepsilon_i + u_i, u_i \\mid \\theta)\\, du_i \n",
    "           = \\int_0^1 g(\\varepsilon_i, t_i \\mid \\theta)\\, dt_i \\notag \\\\\n",
    "         & =  \\mbox{E}[g(\\varepsilon_i, t_i \\mid \\theta)]  \n",
    "       \\approx \\frac{1}{S}\\sum_{s=1}^S g(\\varepsilon_i, t^s \\mid \\theta),\\label{eq:LL_mci}\n",
    "\\end{align}\n",
    "       \n",
    "       \n",
    "where $t_i = h(u_i) \\in (0,\\,1)$, $t^s \\in (0,\\,1)$ is the $s$th element of the sample $\\mathbf{t}^S = (t^1, t^2, \\ldots, t^S)$, and \n",
    "\n",
    "\\begin{aligned}\n",
    "g(\\varepsilon_i, t_i\\mid \\theta) = f_{v,u}\\left(\\varepsilon_i + h^{-1}(t_i),\\, h^{-1}(t_i)\\mid \\theta\\right)\\times J(t_i),\n",
    "\\end{aligned}\n",
    "\n",
    "where $J(\\cdot)$ is the Jacobian. The log-likelihood of the model may be represented by the sum of the log ofÂ \\eqref{eq:LL_mci} over observations. There are a few choices of the transformation function $h(\\cdot)$ (or its inverse function $h^{-1}(\\cdot)$) for the SF model. For instance, $u = t/(1-t)$, to which the Jacobian is $1/(t-1)^2$.\n",
    "\n",
    "Regarding $\\mathbf{t}^S$, we could use pseudo-random numbers from a\n",
    "uniform distribution inÂ $(0,\\,1)$, and the result is typically referred\n",
    "to as the Monte Carlo integration. An alternative and more efficient way\n",
    "is to use quasi-random numbers from a low discrepancy sequenceÂ (such as\n",
    "the Halton sequence). The result is a quasi-Monte Carlo integration. In\n",
    "fact, as we will demonstrate formally in the research, equally-spaced\n",
    "grids inÂ $(0,1)$ could serve the purpose well at least for\n",
    "one-dimensional problemsÂ (cross-sectional models).\n",
    "\n",
    "The above NIA-MCI method has a few advantages over the popular MCSA:\n",
    "\n",
    "-  intuitive: It directly integrates out $u_i$ from the density\n",
    "    function.\n",
    "\n",
    "-  simple: The Halton sequence is widely available. For cross-sectional\n",
    "    models, the Halton sequence may not even be needed; the simple gride\n",
    "    sequence would suffice. In this way, there is no need of specific\n",
    "    softwares or routines.\n",
    "\n",
    "-  applicable to all distributions: Do not require distribution\n",
    "    sampling, thus no need of the inverse CDFs. Only need the PDF of\n",
    "    $v_i$ and $u_i$.\n",
    "\n",
    "To sum up: Both of the NIA-GQ and NIA-MCI have important advantages over\n",
    "the more-popular MCSA. Yet, they are under-utilized in simulation-based\n",
    "estimation, particularly in the field of SF analysis. When used\n",
    "properly, they have the potential for lifting the distribution\n",
    "restrictions on SF models, which is an appealing feature that could not\n",
    "be easily shared with MCSA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
